{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "heated-cigarette",
   "metadata": {},
   "source": [
    "# Project 1 - Sentiment Classification\n",
    "\n",
    "In this project, you will conduct a sentiment analysis task.\n",
    "You will build a model to predict the scores (a.k.a. stars, from 1-5) of each review.\n",
    "For each review, you are given a piece of text as well as some other features (Explore yourself!).\n",
    "You can consider the predicted variables to be categorical, ordinal or numerical.\n",
    "\n",
    "DDL: *April 6, 2021*\n",
    "- *March 23, 2021* release the validation score of weak baseline--60.2%\n",
    "- *March 30, 2021* release the validation score of strong baseline\n",
    "\n",
    "Submission: Each team leader is required to submit the groupNo.zip file in the canvas. It shoud contain \n",
    "- `pre.csv` Predictions on test data (please make sure you can successfully evaluate your validation predictions on the validation data with the help of evaluate.py)\n",
    "- report (1-2 pages of pdf)\n",
    "- code (Frameworks and programming languages are not restricted.)\n",
    "\n",
    "We will check your report with your code and the accuracy.\n",
    "\n",
    "| Grade | Classifier (80%)                                                   | Report (20%)                      |\n",
    "|-------|--------------------------------------------------------------------|-----------------------------------|\n",
    "| 50%   | example code in tutorials or in Project 1 without any modification | submission                        |\n",
    "| 60%   | an easy baseline that most students can outperform                 | algorithm you used                |\n",
    "| 80%   | a competitive baseline that about half students can surpass        | detailed explanation              |\n",
    "| 90%   | a very competitive baseline without any special mechanism          | detailed explanation and analysis, such as explorative data analysis and ablation study |\n",
    "| 100%  | a very competitive baseline with at least one mechanism            | excellent ideas, detailed explanation and solid analysis |\n",
    "\n",
    "\n",
    "## Instruction Content\n",
    "\n",
    "1. Load & Dump the data\n",
    "    1. Load the data\n",
    "    1. Dump the data\n",
    "1. Preprocessing\n",
    "    1. Text data processing recap\n",
    "    1. Explorative data analysis\n",
    "1. Learning Baselines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-fossil",
   "metadata": {},
   "source": [
    "## 1.Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fossil-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import keras as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "# from keras import metricsnmmnbn\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-california",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "Here is a function to load your data, remember put the dataset in the `data_2021_spring` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "weighted-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(split_name='train', columns=['text', 'stars']):\n",
    "    try:\n",
    "        print(f\"select [{', '.join(columns)}] columns from the {split_name} split\")\n",
    "        df = pd.read_csv(f'data_2021_spring/{split_name}.csv')\n",
    "        df = df.loc[:,columns]\n",
    "        print(\"succeed!\")\n",
    "        return df\n",
    "    except:\n",
    "        print(\"Failed, then try to \")\n",
    "        print(f\"select all columns from the {split_name} split\")\n",
    "        df = pd.read_csv(f'data_2021_spring/{split_name}.csv')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cellular-conclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [text, stars, business_id, cool, date, funny, review_id, useful, user_id] columns from the train split\n",
      "succeed!\n"
     ]
    }
   ],
   "source": [
    "train_df = load_data('train', columns=['text',\\\n",
    "                                       'stars','business_id','cool',\\\n",
    "                                       'date','funny','review_id','useful','user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "proper-cruise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, stars, business_id, cool, date, funny, review_id, useful, user_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Items of dataset:\n",
    "    #business_id\n",
    "    #cool: whether it's cool\n",
    "    #date: public date\n",
    "    #funny: whether it's funny\n",
    "    #review_id\n",
    "    #text： content\n",
    "    #useful: whether it's useful\n",
    "    #user_id\n",
    "#load 10000 sentences are training set\n",
    "train_df.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "refined-nebraska",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [text, stars] columns from the test split\n",
      "Failed, then try to \n",
      "select all columns from the test split\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [business_id, cool, date, funny, review_id, text, useful, user_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prepare the test set\n",
    "test_df = load_data('test')\n",
    "test_df.head(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-george",
   "metadata": {},
   "source": [
    "#### Below is the way to write dataset into a .csv file, i.e. the format of submission files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "unnecessary-council",
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct a dataset with genuine review_id and randomized stars\n",
    "random_ans = pd.DataFrame(data={\n",
    "    'review_id': test_df['review_id'],\n",
    "    'stars': np.random.randint(0, 6, size=len(test_df))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "overhead-awareness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b8-ELBwhmDKcmcM8icT86g</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rBpAJhIen_V-zLoXZIcROg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_pALaDG6se9OTkGGhyhnNA</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ru8fpA1Uk0tTFtO5hLM49g</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fRPgwuFoY6SriToXZyaOQA</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id  stars\n",
       "0  b8-ELBwhmDKcmcM8icT86g      2\n",
       "1  rBpAJhIen_V-zLoXZIcROg      5\n",
       "2  _pALaDG6se9OTkGGhyhnNA      4\n",
       "3  ru8fpA1Uk0tTFtO5hLM49g      4\n",
       "4  fRPgwuFoY6SriToXZyaOQA      3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_ans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "expected-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write this dataset into a .csv file, which should be the format of our submission\n",
    "group_number = -1\n",
    "random_ans.to_csv(f'{group_number}-random_ans.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-familiar",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "Preprocessing and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "geological-duplicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#return the lower cases of the texts\n",
    "def lower(s):\n",
    "    \"\"\"\n",
    "    :param s: a string.\n",
    "    return a string with lower characters\n",
    "    Note that we allow the input to be nested string of a list.\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: 'text mining is to identify useful information.'\n",
    "    \"\"\"\n",
    "    if isinstance(s, list):\n",
    "        return [lower(t) for t in s]\n",
    "    if isinstance(s, str):\n",
    "        return s.lower()\n",
    "    else:\n",
    "        raise NotImplementedError(\"unknown datatype\")\n",
    "\n",
    "#tokenize the texts\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     results.append(ps.stem(token))\n",
    "    # return results\n",
    "\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "\n",
    "def n_gram(tokens, n=1):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    :param n: the corresponding n-gram, type: int\n",
    "    return a list of n-gram tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n",
    "    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        return tokens\n",
    "    else:\n",
    "        results = list()\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
    "            results.append(\" \".join(tokens[i:i+n]))\n",
    "        return results\n",
    "    \n",
    "\n",
    "def filter_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of filtered tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     if token not in stopwords and not token.isnumeric():\n",
    "    #         results.append(token)\n",
    "    # return results\n",
    "\n",
    "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]\n",
    "\n",
    "\n",
    "\n",
    "def get_onehot_vector(feats, feats_dict):\n",
    "    \"\"\"\n",
    "    :param data: a list of features, type: list\n",
    "    :param feats_dict: a dict from features to indices, type: dict\n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(len(feats_dict), dtype=np.float)\n",
    "    for f in feats:\n",
    "        # get the feature index, return -1 if the feature is not existed\n",
    "        f_idx = feats_dict.get(f, -1)\n",
    "        if f_idx != -1:\n",
    "            # set the corresponding element as 1\n",
    "            vector[f_idx] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "commercial-korean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [i, took, up, train, union, station, catch, ai...\n",
      "1    [we, worked, fitness, twist, part, best, frien...\n",
      "2    [it, 's, typical, ,, average, ,, run-of-the-mi...\n",
      "3    [we, went, outback, today, celebrate, daughter...\n",
      "4    [we, went, see, nashville, unplugged, country,...\n"
     ]
    }
   ],
   "source": [
    "test_df['tokens'] = test_df['text'].map(tokenize).map(filter_stopwords).map(lower)\n",
    "print(test_df['tokens'].head().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt = \"{:10s},\\t \" * 8\n",
    "\n",
    "for token in doc:\n",
    "    print(fmt.format(token.text, token.lemma_, token.pos_, token.dep_,\n",
    "            token.shape_, str(token.is_alpha), str(token.is_stop), \n",
    "                     str(list(token.children))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-devices",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "searching-special",
   "metadata": {},
   "source": [
    "## 3.Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "engaging-inspector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [text, stars] columns from the train split\n",
      "succeed!\n",
      "select [text, stars] columns from the valid split\n",
      "succeed!\n"
     ]
    }
   ],
   "source": [
    "train_df = load_data('train')[:5000]\n",
    "valid_df = load_data('valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "lesbian-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df['text']\n",
    "y_train = train_df['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "great-brazil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tfidf',\n",
      "                 TfidfVectorizer(tokenizer=<function tokenize at 0x7f01c792c0d0>)),\n",
      "                ('lr', LogisticRegression())])\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=tokenize)\n",
    "lr = LogisticRegression()\n",
    "steps = [('tfidf', tfidf),('lr', lr)]\n",
    "pipe = Pipeline(steps)\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "built-madison",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevyn/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(tokenizer=<function tokenize at 0x7f01c792c0d0>)),\n",
       "                ('lr', LogisticRegression())])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "french-amber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      0.88      0.75       517\n",
      "           2       0.41      0.14      0.21       278\n",
      "           3       0.44      0.47      0.45       344\n",
      "           4       0.50      0.51      0.50       427\n",
      "           5       0.70      0.67      0.68       434\n",
      "\n",
      "    accuracy                           0.58      2000\n",
      "   macro avg       0.54      0.53      0.52      2000\n",
      "weighted avg       0.56      0.58      0.56      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[456  23  21  11   6]\n",
      " [119  38  96  20   5]\n",
      " [ 64  22 160  87  11]\n",
      " [ 22   6  78 217 104]\n",
      " [ 32   3  11  99 289]]\n",
      "accuracy 0.58\n"
     ]
    }
   ],
   "source": [
    "x_valid = valid_df['text']\n",
    "y_valid = valid_df['stars']\n",
    "y_pred = pipe.predict(x_valid)\n",
    "print(classification_report(y_valid, y_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(confusion_matrix(y_valid, y_pred))\n",
    "print('accuracy', np.mean(y_valid == y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "disturbed-logistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_df['text'].map(tokenize).map(filter_stopwords).map(stem)\n",
    "valid_text = valid_df['text'].map(tokenize).map(filter_stopwords).map(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "checked-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {}\n",
    "for tokens in train_text:\n",
    "    for t in tokens:\n",
    "        if not t in word2id:\n",
    "            word2id[t] = len(word2id)\n",
    "word2id['<pad>'] = len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "peaceful-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_id_seq(texts, padding_length=500):\n",
    "    records = []\n",
    "    for tokens in texts:\n",
    "        record = []\n",
    "        for t in tokens:\n",
    "            record.append(word2id.get(t, len(word2id)))\n",
    "        if len(record) >= padding_length:\n",
    "            records.append(record[:padding_length])\n",
    "        else:\n",
    "            records.append(record + [word2id['<pad>']] * (padding_length - len(record)))\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "realistic-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs = texts_to_id_seq(train_text)\n",
    "valid_seqs = texts_to_id_seq(valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "hired-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seq, y):\n",
    "        assert len(seq) == len(y)\n",
    "        self.seq = seq\n",
    "        self.y = y-1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return np.asarray(self.seq[idx]), self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "infinite-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(MyDataset(train_seqs, y_train), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(MyDataset(valid_seqs, y_valid), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "blocked-berkeley",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mlp, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(word2id)+1, embedding_dim=64)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64,\n",
    "                      out_channels=64,\n",
    "                      kernel_size=3,\n",
    "                      stride=1),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=64,\n",
    "                      out_channels=64,\n",
    "                      kernel_size=3,\n",
    "                      stride=1),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.linear = nn.Linear(64, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = self.cnn(x)\n",
    "        x = torch.max(x, dim=-1)[0]\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "coordinated-fossil",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlp()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "arranged-township",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/313 [00:00<00:32,  9.53it/s, loss=0.11, acc=0.167]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:16<00:00, 18.79it/s, loss=0.0961, acc=0.311]\n",
      "100%|██████████| 125/125 [00:01<00:00, 82.00it/s]\n",
      "  1%|          | 2/313 [00:00<00:20, 15.14it/s, loss=0.0784, acc=0.594]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.72      0.57       517\n",
      "           1       0.00      0.00      0.00       278\n",
      "           2       0.30      0.44      0.36       344\n",
      "           3       0.40      0.12      0.18       427\n",
      "           4       0.46      0.61      0.53       434\n",
      "\n",
      "    accuracy                           0.42      2000\n",
      "   macro avg       0.33      0.38      0.33      2000\n",
      "weighted avg       0.36      0.42      0.36      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[373   1  60   6  77]\n",
      " [139   0  88   9  42]\n",
      " [116   0 153  23  52]\n",
      " [ 90   0 154  50 133]\n",
      " [ 72   0  60  38 264]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:15<00:00, 19.99it/s, loss=0.0817, acc=0.461]\n",
      "100%|██████████| 125/125 [00:01<00:00, 86.31it/s]\n",
      "  1%|          | 3/313 [00:00<00:13, 22.26it/s, loss=0.0702, acc=0.484]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.63      0.64       517\n",
      "           1       0.30      0.11      0.16       278\n",
      "           2       0.28      0.71      0.40       344\n",
      "           3       0.43      0.07      0.13       427\n",
      "           4       0.53      0.53      0.53       434\n",
      "\n",
      "    accuracy                           0.43      2000\n",
      "   macro avg       0.43      0.41      0.37      2000\n",
      "weighted avg       0.46      0.43      0.40      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[328  36 119   1  33]\n",
      " [ 71  31 151   5  20]\n",
      " [ 36  19 243  12  34]\n",
      " [ 33  13 232  32 117]\n",
      " [ 48   6 127  24 229]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:15<00:00, 19.95it/s, loss=0.0688, acc=0.558]\n",
      "100%|██████████| 125/125 [00:01<00:00, 94.13it/s]\n",
      "  1%|          | 3/313 [00:00<00:13, 22.15it/s, loss=0.0621, acc=0.609]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.57      0.64       517\n",
      "           1       0.28      0.30      0.29       278\n",
      "           2       0.36      0.13      0.20       344\n",
      "           3       0.32      0.72      0.45       427\n",
      "           4       0.67      0.32      0.44       434\n",
      "\n",
      "    accuracy                           0.44      2000\n",
      "   macro avg       0.47      0.41      0.40      2000\n",
      "weighted avg       0.50      0.44      0.43      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[297 106  15  85  14]\n",
      " [ 59  84  22 108   5]\n",
      " [ 22  65  46 203   8]\n",
      " [ 15  28  36 307  41]\n",
      " [ 25  12   8 248 141]]\n",
      "epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:15<00:00, 20.01it/s, loss=0.0574, acc=0.643]\n",
      "100%|██████████| 125/125 [00:01<00:00, 92.16it/s]\n",
      "  1%|          | 3/313 [00:00<00:14, 21.15it/s, loss=0.0432, acc=0.75] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.64      0.66       517\n",
      "           1       0.28      0.41      0.33       278\n",
      "           2       0.38      0.30      0.33       344\n",
      "           3       0.37      0.35      0.36       427\n",
      "           4       0.55      0.56      0.55       434\n",
      "\n",
      "    accuracy                           0.47      2000\n",
      "   macro avg       0.45      0.45      0.45      2000\n",
      "weighted avg       0.48      0.47      0.47      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[329 110  29  16  33]\n",
      " [ 69 113  39  33  24]\n",
      " [ 36  95 103  88  22]\n",
      " [ 15  60  80 150 122]\n",
      " [ 25  31  22 114 242]]\n",
      "epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:15<00:00, 20.16it/s, loss=0.0451, acc=0.724]\n",
      "100%|██████████| 125/125 [00:01<00:00, 91.99it/s]\n",
      "  1%|          | 3/313 [00:00<00:12, 25.02it/s, loss=0.0291, acc=0.906]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.69      0.69       517\n",
      "           1       0.30      0.33      0.31       278\n",
      "           2       0.36      0.32      0.34       344\n",
      "           3       0.35      0.52      0.42       427\n",
      "           4       0.63      0.35      0.45       434\n",
      "\n",
      "    accuracy                           0.47      2000\n",
      "   macro avg       0.47      0.44      0.44      2000\n",
      "weighted avg       0.49      0.47      0.47      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[359  80  29  33  16]\n",
      " [ 84  91  47  51   5]\n",
      " [ 33  79 111 113   8]\n",
      " [ 14  37  94 220  62]\n",
      " [ 28  18  27 208 153]]\n",
      "epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:15<00:00, 19.66it/s, loss=0.0338, acc=0.811]\n",
      "100%|██████████| 125/125 [00:01<00:00, 89.09it/s]\n",
      "  1%|          | 2/313 [00:00<00:18, 16.97it/s, loss=0.0303, acc=0.833]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.79      0.69       517\n",
      "           1       0.32      0.16      0.21       278\n",
      "           2       0.33      0.42      0.37       344\n",
      "           3       0.36      0.33      0.34       427\n",
      "           4       0.58      0.49      0.53       434\n",
      "\n",
      "    accuracy                           0.47      2000\n",
      "   macro avg       0.44      0.44      0.43      2000\n",
      "weighted avg       0.46      0.47      0.46      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[407  27  42  17  24]\n",
      " [116  44  77  32   9]\n",
      " [ 67  38 145  76  18]\n",
      " [ 32  19 134 140 102]\n",
      " [ 46  10  43 124 211]]\n",
      "epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:16<00:00, 18.43it/s, loss=0.0228, acc=0.873]\n",
      "100%|██████████| 125/125 [00:01<00:00, 69.47it/s]\n",
      "  1%|          | 2/313 [00:00<00:17, 17.56it/s, loss=0.0143, acc=0.958]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.62      0.66       517\n",
      "           1       0.29      0.28      0.29       278\n",
      "           2       0.31      0.48      0.38       344\n",
      "           3       0.37      0.35      0.36       427\n",
      "           4       0.60      0.46      0.52       434\n",
      "\n",
      "    accuracy                           0.46      2000\n",
      "   macro avg       0.46      0.44      0.44      2000\n",
      "weighted avg       0.49      0.46      0.47      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[322  92  59  18  26]\n",
      " [ 69  79  89  29  12]\n",
      " [ 28  59 165  77  15]\n",
      " [ 11  29 158 151  78]\n",
      " [ 23  15  62 134 200]]\n",
      "epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:16<00:00, 19.25it/s, loss=0.0143, acc=0.931]\n",
      "100%|██████████| 125/125 [00:01<00:00, 93.47it/s]\n",
      "  1%|          | 2/313 [00:00<00:21, 14.20it/s, loss=0.0087, acc=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.74      0.69       517\n",
      "           1       0.32      0.10      0.16       278\n",
      "           2       0.34      0.51      0.41       344\n",
      "           3       0.36      0.25      0.30       427\n",
      "           4       0.52      0.57      0.54       434\n",
      "\n",
      "    accuracy                           0.47      2000\n",
      "   macro avg       0.43      0.44      0.42      2000\n",
      "weighted avg       0.46      0.47      0.45      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[384  18  56  15  44]\n",
      " [111  29  99  21  18]\n",
      " [ 51  27 177  61  28]\n",
      " [ 24  13 141 108 141]\n",
      " [ 30   5  51  99 249]]\n",
      "epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:16<00:00, 19.53it/s, loss=0.00815, acc=0.972]\n",
      "100%|██████████| 125/125 [00:01<00:00, 91.02it/s]\n",
      "  1%|          | 3/313 [00:00<00:14, 21.27it/s, loss=0.00471, acc=0.979]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.76      0.67       517\n",
      "           1       0.29      0.15      0.20       278\n",
      "           2       0.31      0.52      0.39       344\n",
      "           3       0.33      0.25      0.28       427\n",
      "           4       0.61      0.41      0.49       434\n",
      "\n",
      "    accuracy                           0.45      2000\n",
      "   macro avg       0.43      0.42      0.41      2000\n",
      "weighted avg       0.45      0.45      0.44      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[394  32  57  14  20]\n",
      " [111  42  92  22  11]\n",
      " [ 58  41 180  52  13]\n",
      " [ 35  23 192 106  71]\n",
      " [ 55   8  69 123 179]]\n",
      "epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:14<00:00, 21.12it/s, loss=0.00492, acc=0.987]\n",
      "100%|██████████| 125/125 [00:01<00:00, 89.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.73      0.67       517\n",
      "           1       0.31      0.19      0.24       278\n",
      "           2       0.33      0.41      0.36       344\n",
      "           3       0.37      0.43      0.40       427\n",
      "           4       0.61      0.39      0.47       434\n",
      "\n",
      "    accuracy                           0.46      2000\n",
      "   macro avg       0.44      0.43      0.43      2000\n",
      "weighted avg       0.47      0.46      0.46      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[379  49  41  30  18]\n",
      " [106  53  75  33  11]\n",
      " [ 57  40 140  93  14]\n",
      " [ 30  18 128 185  66]\n",
      " [ 47  11  45 163 168]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for e in range(1, 11):    \n",
    "    print('epoch', e)\n",
    "    model.train()\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    with tqdm.tqdm(train_loader) as t:\n",
    "        for x, y in t:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            total_acc += (logits.argmax(1) == y).sum().item()\n",
    "            total_count += y.size(0)\n",
    "            total_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            t.set_postfix({'loss': total_loss/total_count, 'acc': total_acc/total_count})\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    with tqdm.tqdm(valid_loader) as t:\n",
    "        for x, y in t:\n",
    "            logits = model(x)\n",
    "            total_acc += (logits.argmax(1) == y).sum().item()\n",
    "            total_count += len(y)\n",
    "            y_pred += logits.argmax(1).tolist()\n",
    "            y_true += y.tolist()\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"\\n\\n\")\n",
    "    print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-purchase",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
