{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "heated-cigarette",
   "metadata": {},
   "source": [
    "# Project 1 - Sentiment Classification\n",
    "\n",
    "In this project, you will conduct a sentiment analysis task.\n",
    "You will build a model to predict the scores (a.k.a. stars, from 1-5) of each review.\n",
    "For each review, you are given a piece of text as well as some other features (Explore yourself!).\n",
    "You can consider the predicted variables to be categorical, ordinal or numerical.\n",
    "\n",
    "DDL: *April 6, 2021*\n",
    "- *March 23, 2021* release the validation score of weak baseline--60.2%\n",
    "- *March 30, 2021* release the validation score of strong baseline\n",
    "\n",
    "Submission: Each team leader is required to submit the groupNo.zip file in the canvas. It shoud contain \n",
    "- `pre.csv` Predictions on test data (please make sure you can successfully evaluate your validation predictions on the validation data with the help of evaluate.py)\n",
    "- report (1-2 pages of pdf)\n",
    "- code (Frameworks and programming languages are not restricted.)\n",
    "\n",
    "We will check your report with your code and the accuracy.\n",
    "\n",
    "| Grade | Classifier (80%)                                                   | Report (20%)                      |\n",
    "|-------|--------------------------------------------------------------------|-----------------------------------|\n",
    "| 50%   | example code in tutorials or in Project 1 without any modification | submission                        |\n",
    "| 60%   | an easy baseline that most students can outperform                 | algorithm you used                |\n",
    "| 80%   | a competitive baseline that about half students can surpass        | detailed explanation              |\n",
    "| 90%   | a very competitive baseline without any special mechanism          | detailed explanation and analysis, such as explorative data analysis and ablation study |\n",
    "| 100%  | a very competitive baseline with at least one mechanism            | excellent ideas, detailed explanation and solid analysis |\n",
    "\n",
    "\n",
    "## Instruction Content\n",
    "\n",
    "1. Load & Dump the data\n",
    "    1. Load the data\n",
    "    1. Dump the data\n",
    "1. Preprocessing\n",
    "    1. Text data processing recap\n",
    "    1. Explorative data analysis\n",
    "1. Learning Baselines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-fossil",
   "metadata": {},
   "source": [
    "## 1.Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fossil-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import keras as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "# from keras import metricsnmmnbn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-california",
   "metadata": {},
   "source": [
    "### A. Load the data\n",
    "\n",
    "Here is a function to load your data, remember put the dataset in the `data_2021_spring` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "weighted-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(split_name='train', columns=['text', 'stars']):\n",
    "    try:\n",
    "        print(f\"select [{', '.join(columns)}] columns from the {split_name} split\")\n",
    "        df = pd.read_csv(f'data_2021_spring/{split_name}.csv')\n",
    "        df = df.loc[:,columns]\n",
    "        print(\"succeed!\")\n",
    "        return df\n",
    "    except:\n",
    "        print(\"Failed, then try to \")\n",
    "        print(f\"select all columns from the {split_name} split\")\n",
    "        df = pd.read_csv(f'data_2021_spring/{split_name}.csv')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cellular-conclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [text, stars, business_id, cool, date, funny, review_id, useful, user_id] columns from the train split\n",
      "succeed!\n"
     ]
    }
   ],
   "source": [
    "train_df = load_data('train', columns=['text',\\\n",
    "                                       'stars','business_id','cool',\\\n",
    "                                       'date','funny','review_id','useful','user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "proper-cruise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, stars, business_id, cool, date, funny, review_id, useful, user_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Items of dataset:\n",
    "    #business_id\n",
    "    #cool: whether it's cool\n",
    "    #date: public date\n",
    "    #funny: whether it's funny\n",
    "    #review_id\n",
    "    #textï¼š content\n",
    "    #useful: whether it's useful\n",
    "    #user_id\n",
    "#load 10000 sentences are training set\n",
    "train_df.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "refined-nebraska",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [text, stars] columns from the test split\n",
      "Failed, then try to \n",
      "select all columns from the test split\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [business_id, cool, date, funny, review_id, text, useful, user_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prepare the test set\n",
    "test_df = load_data('test')\n",
    "test_df.head(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-george",
   "metadata": {},
   "source": [
    "#### Below is the way to write dataset into a .csv file, i.e. the format of submission files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "unnecessary-council",
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct a dataset with genuine review_id and randomized stars\n",
    "random_ans = pd.DataFrame(data={\n",
    "    'review_id': test_df['review_id'],\n",
    "    'stars': np.random.randint(0, 6, size=len(test_df))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "overhead-awareness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b8-ELBwhmDKcmcM8icT86g</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rBpAJhIen_V-zLoXZIcROg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_pALaDG6se9OTkGGhyhnNA</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ru8fpA1Uk0tTFtO5hLM49g</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fRPgwuFoY6SriToXZyaOQA</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id  stars\n",
       "0  b8-ELBwhmDKcmcM8icT86g      2\n",
       "1  rBpAJhIen_V-zLoXZIcROg      5\n",
       "2  _pALaDG6se9OTkGGhyhnNA      4\n",
       "3  ru8fpA1Uk0tTFtO5hLM49g      4\n",
       "4  fRPgwuFoY6SriToXZyaOQA      3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_ans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "expected-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write this dataset into a .csv file, which should be the format of our submission\n",
    "group_number = -1\n",
    "random_ans.to_csv(f'{group_number}-random_ans.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-familiar",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "Preprocessing and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "geological-duplicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#return the lower cases of the texts\n",
    "def lower(s):\n",
    "    \"\"\"\n",
    "    :param s: a string.\n",
    "    return a string with lower characters\n",
    "    Note that we allow the input to be nested string of a list.\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: 'text mining is to identify useful information.'\n",
    "    \"\"\"\n",
    "    if isinstance(s, list):\n",
    "        return [lower(t) for t in s]\n",
    "    if isinstance(s, str):\n",
    "        return s.lower()\n",
    "    else:\n",
    "        raise NotImplementedError(\"unknown datatype\")\n",
    "\n",
    "#tokenize the texts\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     results.append(ps.stem(token))\n",
    "    # return results\n",
    "\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "\n",
    "def n_gram(tokens, n=1):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    :param n: the corresponding n-gram, type: int\n",
    "    return a list of n-gram tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n",
    "    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        return tokens\n",
    "    else:\n",
    "        results = list()\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
    "            results.append(\" \".join(tokens[i:i+n]))\n",
    "        return results\n",
    "    \n",
    "\n",
    "def filter_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of filtered tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     if token not in stopwords and not token.isnumeric():\n",
    "    #         results.append(token)\n",
    "    # return results\n",
    "\n",
    "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]\n",
    "\n",
    "\n",
    "\n",
    "def get_onehot_vector(feats, feats_dict):\n",
    "    \"\"\"\n",
    "    :param data: a list of features, type: list\n",
    "    :param feats_dict: a dict from features to indices, type: dict\n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(len(feats_dict), dtype=np.float)\n",
    "    for f in feats:\n",
    "        # get the feature index, return -1 if the feature is not existed\n",
    "        f_idx = feats_dict.get(f, -1)\n",
    "        if f_idx != -1:\n",
    "            # set the corresponding element as 1\n",
    "            vector[f_idx] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "commercial-korean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [i, took, up, train, union, station, catch, ai...\n",
      "1    [we, worked, fitness, twist, part, best, frien...\n",
      "2    [it, 's, typical, ,, average, ,, run-of-the-mi...\n",
      "3    [we, went, outback, today, celebrate, daughter...\n",
      "4    [we, went, see, nashville, unplugged, country,...\n"
     ]
    }
   ],
   "source": [
    "test_df['tokens'] = test_df['text'].map(tokenize).map(filter_stopwords).map(lower)\n",
    "print(test_df['tokens'].head().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-extent",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
