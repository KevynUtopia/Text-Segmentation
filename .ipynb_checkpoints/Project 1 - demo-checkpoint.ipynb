{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions for Project 1 - Sentiment Classification\n",
    "\n",
    "Hello everyone, this is Zihao. I am very happy to host the first project\n",
    "\n",
    "In this project, you will conduct a sentiment analysis task.\n",
    "You will build a model to predict the scores (a.k.a. stars, from 1-5) of each review.\n",
    "For each review, you are given a piece of text as well as some other features (Explore yourself!).\n",
    "You can consider the predicted variables to be categorical, ordinal or numerical.\n",
    "\n",
    "DDL: *April 6, 2021*\n",
    "- *March 23, 2021* release the validation score of weak baseline\n",
    "- *March 30, 2021* release the validation score of strong baseline\n",
    "\n",
    "Submission: Each team leader is required to submit the groupNo.zip file in the canvas. It shoud contain \n",
    "- `pre.csv` Predictions on test data (please make sure you can successfully evaluate your validation predictions on the validation data with the help of evaluate.py)\n",
    "- report (1-2 pages of pdf)\n",
    "- code (Frameworks and programming languages are not restricted.)\n",
    "\n",
    "We will check your report with your code and the accuracy.\n",
    "\n",
    "| Grade | Classifier (80%)                                                   | Report (20%)                      |\n",
    "|-------|--------------------------------------------------------------------|-----------------------------------|\n",
    "| 50%   | example code in tutorials or in Project 1 without any modification | submission                        |\n",
    "| 60%   | an easy baseline that most students can outperform                 | algorithm you used                |\n",
    "| 80%   | a competitive baseline that about half students can surpass        | detailed explanation              |\n",
    "| 90%   | a very competitive baseline without any special mechanism          | detailed explanation and analysis, such as explorative data analysis and ablation study |\n",
    "| 100%  | a very competitive baseline with at least one mechanism            | excellent ideas, detailed explanation and solid analysis |\n",
    "\n",
    "\n",
    "\n",
    "In this notebook, you are provided with the code snippets for you to start.\n",
    "\n",
    "The content follows previous lectures and tutorials. But I may mention some useful python packages.\n",
    "\n",
    "## Instruction Content\n",
    "\n",
    "1. Load & Dump the data\n",
    "    1. Load the data\n",
    "    1. Dump the data\n",
    "1. Preprocessing\n",
    "    1. Text data processing recap\n",
    "    1. Explorative data analysis\n",
    "1. Learning Baselines\n",
    "\n",
    "## 1. Load & Dump the data\n",
    "\n",
    "The same as previous tutorials, we use `pandas` as the basic tool to load & dump the data.\n",
    "The key ingredient of our operation is the `DataFrame` in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Load the data\n",
    "\n",
    "Here is a function to load your data, remember put the dataset in the `data_2021_spring` folder.\n",
    "\n",
    "Each year we release different data, so old models are not guaranteed to solve the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(split_name='train', columns=['text', 'stars']):\n",
    "    try:\n",
    "        print(f\"select [{', '.join(columns)}] columns from the {split_name} split\")\n",
    "        df = pd.read_csv(f'data_2021_spring/{split_name}.csv')\n",
    "        df = df.loc[:,columns]\n",
    "        print(\"succeed!\")\n",
    "        return df\n",
    "    except:\n",
    "        print(\"Failed, then try to \")\n",
    "        print(f\"select all columns from the {split_name} split\")\n",
    "        df = pd.read_csv(f'data_2021_spring/{split_name}.csv')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can extract the data by specifying the desired split and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [text] columns from the train split\n",
      "succeed!\n"
     ]
    }
   ],
   "source": [
    "train_df = load_data('train', columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nice to have a diner still around. Food was go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tried this a while back, got the fried chicken...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I expected more pork selections on menu. Food ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YUMMY!!! This place is phenomenal. It is Price...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Truffle Macaroni &amp; Cheese and Potatoes Au ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Nice to have a diner still around. Food was go...\n",
       "1  Tried this a while back, got the fried chicken...\n",
       "2  I expected more pork selections on menu. Food ...\n",
       "3  YUMMY!!! This place is phenomenal. It is Price...\n",
       "4  The Truffle Macaroni & Cheese and Potatoes Au ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [text, stars] columns from the test split\n",
      "Failed, then try to \n",
      "select all columns from the test split\n"
     ]
    }
   ],
   "source": [
    "test_df = load_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Dump the random answer\n",
    "\n",
    "In this project, your predictions on test data are supposed to be submitted by a csv file of two columns, i.e. (review_id and stars)\n",
    "\n",
    "Here we compose the random answer in a DataFrame and dump the answer into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ans = pd.DataFrame(data={\n",
    "    'review_id': test_df['review_id'],\n",
    "    'stars': np.random.randint(0, 6, size=len(test_df))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b8-ELBwhmDKcmcM8icT86g</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rBpAJhIen_V-zLoXZIcROg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_pALaDG6se9OTkGGhyhnNA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ru8fpA1Uk0tTFtO5hLM49g</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fRPgwuFoY6SriToXZyaOQA</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id  stars\n",
       "0  b8-ELBwhmDKcmcM8icT86g      3\n",
       "1  rBpAJhIen_V-zLoXZIcROg      5\n",
       "2  _pALaDG6se9OTkGGhyhnNA      0\n",
       "3  ru8fpA1Uk0tTFtO5hLM49g      4\n",
       "4  fRPgwuFoY6SriToXZyaOQA      3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_ans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_number = -1\n",
    "random_ans.to_csv(f'{group_number}-random_ans.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "Preprocessing and feature engineering is important in machine learning\n",
    "\n",
    "### A. Text data processing recap\n",
    "In our tutorials, Haoran have showed you how to extract textual features by the `nltk` package\n",
    "\n",
    "Remember to use the NLTK Downloader to obtain the resource:\n",
    "```\n",
    "  >>> import nltk\n",
    "  >>> nltk.download('stopwords')\n",
    "  >>> nltk.download('punkt')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def lower(s):\n",
    "    \"\"\"\n",
    "    :param s: a string.\n",
    "    return a string with lower characters\n",
    "    Note that we allow the input to be nested string of a list.\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: 'text mining is to identify useful information.'\n",
    "    \"\"\"\n",
    "    if isinstance(s, list):\n",
    "        return [lower(t) for t in s]\n",
    "    if isinstance(s, str):\n",
    "        return s.lower()\n",
    "    else:\n",
    "        raise NotImplementedError(\"unknown datatype\")\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     results.append(ps.stem(token))\n",
    "    # return results\n",
    "\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "\n",
    "def n_gram(tokens, n=1):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    :param n: the corresponding n-gram, type: int\n",
    "    return a list of n-gram tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n",
    "    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        return tokens\n",
    "    else:\n",
    "        results = list()\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
    "            results.append(\" \".join(tokens[i:i+n]))\n",
    "        return results\n",
    "\n",
    "def filter_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of filtered tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     if token not in stopwords and not token.isnumeric():\n",
    "    #         results.append(token)\n",
    "    # return results\n",
    "\n",
    "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_onehot_vector(feats, feats_dict):\n",
    "    \"\"\"\n",
    "    :param data: a list of features, type: list\n",
    "    :param feats_dict: a dict from features to indices, type: dict\n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(len(feats_dict), dtype=np.float)\n",
    "    for f in feats:\n",
    "        # get the feature index, return -1 if the feature is not existed\n",
    "        f_idx = feats_dict.get(f, -1)\n",
    "        if f_idx != -1:\n",
    "            # set the corresponding element as 1\n",
    "            vector[f_idx] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can use the `map` function to apply your preprocessing functions into the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [i, took, up, train, union, station, catch, ai...\n",
      "1    [we, worked, fitness, twist, part, best, frien...\n",
      "2    [it, 's, typical, ,, average, ,, run-of-the-mi...\n",
      "3    [we, went, outback, today, celebrate, daughter...\n",
      "4    [we, went, see, nashville, unplugged, country,...\n"
     ]
    }
   ],
   "source": [
    "test_df['tokens'] = test_df['text'].map(tokenize).map(filter_stopwords).map(lower)\n",
    "print(test_df['tokens'].head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides `nltk`, I would like to introduce `SpaCy`, a newer text processing toolkit of industrial strength.\n",
    "\n",
    "You can explore it at https://spacy.io/\n",
    "\n",
    "Let's install it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "python -m pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy enables you use linguistic features of texts\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw       ,\t stem      ,\t PartOfSpeech,\t dependency,\t shape     ,\t is alpha  ,\t is stop   ,\t its childrens in the parsing tree,\t \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Apple     ,\t Apple     ,\t PROPN     ,\t nsubj     ,\t Xxxxx     ,\t True      ,\t False     ,\t []        ,\t \n",
      "is        ,\t be        ,\t AUX       ,\t aux       ,\t xx        ,\t True      ,\t True      ,\t []        ,\t \n",
      "looking   ,\t look      ,\t VERB      ,\t ROOT      ,\t xxxx      ,\t True      ,\t False     ,\t [Apple, is, at, startup],\t \n",
      "at        ,\t at        ,\t ADP       ,\t prep      ,\t xx        ,\t True      ,\t True      ,\t [buying]  ,\t \n",
      "buying    ,\t buy       ,\t VERB      ,\t pcomp     ,\t xxxx      ,\t True      ,\t False     ,\t [U.K.]    ,\t \n",
      "U.K.      ,\t U.K.      ,\t PROPN     ,\t dobj      ,\t X.X.      ,\t False     ,\t False     ,\t []        ,\t \n",
      "startup   ,\t startup   ,\t NOUN      ,\t advcl     ,\t xxxx      ,\t True      ,\t False     ,\t [for]     ,\t \n",
      "for       ,\t for       ,\t ADP       ,\t prep      ,\t xxx       ,\t True      ,\t True      ,\t [billion] ,\t \n",
      "$         ,\t $         ,\t SYM       ,\t quantmod  ,\t $         ,\t False     ,\t False     ,\t []        ,\t \n",
      "1         ,\t 1         ,\t NUM       ,\t compound  ,\t d         ,\t False     ,\t False     ,\t []        ,\t \n",
      "billion   ,\t billion   ,\t NUM       ,\t pobj      ,\t xxxx      ,\t True      ,\t False     ,\t [$, 1]    ,\t \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "fmt = \"{:10s},\\t \" * 8\n",
    "print(fmt.format('raw', 'stem', 'PartOfSpeech', 'dependency', 'shape', 'is alpha', 'is stop', 'its childrens in the parsing tree'))\n",
    "print('-'*140)\n",
    "for token in doc:\n",
    "    print(fmt.format(token.text, token.lemma_, token.pos_, token.dep_,\n",
    "            token.shape_, str(token.is_alpha), str(token.is_stop), str(list(token.children))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy also allows you use the embeddings for both sentence and words\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple is looking at buying U.K. startup for $1 billion [ 0.4847193   0.34561655  0.23650904 -0.27294627  0.30828613] ...\n",
      "Apple [ 0.9396687   0.46727175 -0.3862503  -0.23296848  0.25683203] ...\n",
      "is [-0.21470308 -0.36800703  1.8618155  -0.43874717 -0.6448474 ] ...\n",
      "looking [ 1.5960355  -0.01218066 -0.1948367   0.7979922   0.36900565] ...\n",
      "at [-1.2617028  -0.8116296  -0.55736023  0.08604071 -0.43663728] ...\n",
      "buying [ 0.3020423  -0.9611639   1.2695026   0.10633498  2.8583994 ] ...\n",
      "U.K. [ 2.2959712   0.78135234 -1.0174923  -0.5566485   0.69199914] ...\n",
      "startup [0.6782811  0.03798376 0.07798427 0.1210558  0.5636424 ] ...\n",
      "for [-0.07904667 -0.21996386 -1.3529027  -0.24131706  0.43687835] ...\n",
      "$ [ 0.44878927  0.75564337  0.5757578  -1.1713823   0.7438692 ] ...\n",
      "1 [-0.3846085  2.7049747  2.7081459 -1.4393395 -0.5412608] ...\n",
      "billion [ 1.011186    1.4275012  -0.38276425 -0.03342953 -0.9067332 ] ...\n"
     ]
    }
   ],
   "source": [
    "print(doc, doc.vector[:5], '...')\n",
    "for t in doc:\n",
    "    print(t, t.vector[:5], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more usage of SpaCy, you can refer to the documentation of spacy https://spacy.io/usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Explorative data analysis\n",
    "\n",
    "For our dataset, we have features more than text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [f, u, l, l] columns from the train split\n",
      "Failed, then try to \n",
      "select all columns from the train split\n"
     ]
    }
   ],
   "source": [
    "train_df_full = load_data('train', columns='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39rLHYJOy2774ZIUouuWLw</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-06-28 21:44:02</td>\n",
       "      <td>0</td>\n",
       "      <td>ynzOFepQYSCDGdfWDWxiZw</td>\n",
       "      <td>4</td>\n",
       "      <td>Nice to have a diner still around. Food was go...</td>\n",
       "      <td>0</td>\n",
       "      <td>Sl6VgFOB-XXfFIAYp7TFkw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E-Kq1Yu1d6N3TL2qX0aqjA</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-09 03:45:19</td>\n",
       "      <td>0</td>\n",
       "      <td>sQX9ncJBEdBf16AWsvO6Vg</td>\n",
       "      <td>2</td>\n",
       "      <td>Tried this a while back, got the fried chicken...</td>\n",
       "      <td>0</td>\n",
       "      <td>gcx01pMqWzkni2UC-zoZrA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nWW6fBfBljiRFa4sG7TyxA</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-07-19 02:00:04</td>\n",
       "      <td>0</td>\n",
       "      <td>bVIf2kqbzvif3miNe3ARNw</td>\n",
       "      <td>4</td>\n",
       "      <td>I expected more pork selections on menu. Food ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Mn9VzPbrCYU4EcP_C1oBOg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qmIHO-6T_KEfPC9jyGDamQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-11-11 08:10:24</td>\n",
       "      <td>0</td>\n",
       "      <td>LNj1OFxy2ool3PZANGchPA</td>\n",
       "      <td>4</td>\n",
       "      <td>YUMMY!!! This place is phenomenal. It is Price...</td>\n",
       "      <td>0</td>\n",
       "      <td>SKV1heo00fdciCbCN9Z33A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pKk7jCFIm96qDdk0laVT2w</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-01-16 20:04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>bZXxa0hO6wQlHD-MkMf4iw</td>\n",
       "      <td>5</td>\n",
       "      <td>The Truffle Macaroni &amp; Cheese and Potatoes Au ...</td>\n",
       "      <td>1</td>\n",
       "      <td>p1r7rZYruZR92x1A649PTQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                 date  funny  \\\n",
       "0  39rLHYJOy2774ZIUouuWLw     0  2017-06-28 21:44:02      0   \n",
       "1  E-Kq1Yu1d6N3TL2qX0aqjA     0  2018-04-09 03:45:19      0   \n",
       "2  nWW6fBfBljiRFa4sG7TyxA     0  2014-07-19 02:00:04      0   \n",
       "3  qmIHO-6T_KEfPC9jyGDamQ     0  2011-11-11 08:10:24      0   \n",
       "4  pKk7jCFIm96qDdk0laVT2w     1  2010-01-16 20:04:00      1   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  ynzOFepQYSCDGdfWDWxiZw      4   \n",
       "1  sQX9ncJBEdBf16AWsvO6Vg      2   \n",
       "2  bVIf2kqbzvif3miNe3ARNw      4   \n",
       "3  LNj1OFxy2ool3PZANGchPA      4   \n",
       "4  bZXxa0hO6wQlHD-MkMf4iw      5   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  Nice to have a diner still around. Food was go...       0   \n",
       "1  Tried this a while back, got the fried chicken...       0   \n",
       "2  I expected more pork selections on menu. Food ...       0   \n",
       "3  YUMMY!!! This place is phenomenal. It is Price...       0   \n",
       "4  The Truffle Macaroni & Cheese and Potatoes Au ...       1   \n",
       "\n",
       "                  user_id  \n",
       "0  Sl6VgFOB-XXfFIAYp7TFkw  \n",
       "1  gcx01pMqWzkni2UC-zoZrA  \n",
       "2  Mn9VzPbrCYU4EcP_C1oBOg  \n",
       "3  SKV1heo00fdciCbCN9Z33A  \n",
       "4  p1r7rZYruZR92x1A649PTQ  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can explore the relationship between different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fcf74013220>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(train_df_full['cool'], train_df_full['funny'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2534., 1354., 1888., 2110., 2114.]),\n",
       " array([1. , 1.8, 2.6, 3.4, 4.2, 5. ]),\n",
       " <BarContainer object of 5 artists>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_df_full['stars'], bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, you may use the id feature to aggregate data samples\n",
    "\n",
    "For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 business_id  funny  cool  stars\n",
      "7043  -0qht1roIqleKiQkBLDkbw      1     0      3\n",
      "7363  -0qht1roIqleKiQkBLDkbw      0     0      5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAO0ElEQVR4nO3df6zdd13H8eeLloEKwrBXs/TH2miJVCRu3swlGJ2CoRuk1WBMa4hAJo26ocmIZgQzdfwDLhFCnEKDhIGBUpZgKhQngRGM2rG7ANN2KVzLZK0mK9sgIYvMwds/zrd4enZvz/e233tu9+H5SE76/fG557z27ee+eu73e893qSokSU9/z1jrAJKkYVjoktQIC12SGmGhS1IjLHRJasT6tXrhDRs21NatW9fq5SXpaem+++77elXNLbVvzQp969atLCwsrNXLS9LTUpL/XG6fp1wkqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSI6YWepL3JXk4yb8vsz9J3pVkMcn9Sa4cPqYkaZo+79DfD+w8x/5rge3dYx/w1xceS5K0UlMLvao+Bzx6jiG7gQ/UyBHg+UkuGyqgJKmfIT4puhF4aGz9ZLftvycHJtnH6F08W7ZsOe8X3HrzJ877a5+uHnzbK9c6grQq/H4ezkwvilbV/qqar6r5ubklb0UgSTpPQxT6KWDz2PqmbpskaYaGKPRDwG91v+1yNfDNqnrK6RZJ0uqaeg49yYeBa4ANSU4CfwI8E6Cq3g0cBq4DFoHHgdevVlhJ0vKmFnpV7Z2yv4AbBkskSTovflJUkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1IhehZ5kZ5LjSRaT3LzE/i1J7k7yhST3J7lu+KiSpHOZWuhJ1gG3A9cCO4C9SXZMDPtj4GBVXQHsAf5q6KCSpHPr8w79KmCxqk5U1RPAAWD3xJgCfrhbfh7wX8NFlCT10afQNwIPja2f7LaN+1PgNUlOAoeBNy71REn2JVlIsnD69OnziCtJWs5QF0X3Au+vqk3AdcAHkzzluatqf1XNV9X83NzcQC8tSYJ+hX4K2Dy2vqnbNu564CBAVf0r8GxgwxABJUn99Cn0e4HtSbYluYTRRc9DE2O+BrwMIMmLGBW651QkaYamFnpVPQncCNwFPMDot1mOJrk1ya5u2JuANyT5EvBh4HVVVasVWpL0VOv7DKqqw4wudo5vu2Vs+Rjw0mGjSZJWwk+KSlIjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhrRq9CT7ExyPMlikpuXGfMbSY4lOZrkQ8PGlCRNs37agCTrgNuBXwFOAvcmOVRVx8bGbAfeDLy0qh5L8qOrFViStLQ+79CvAhar6kRVPQEcAHZPjHkDcHtVPQZQVQ8PG1OSNE2fQt8IPDS2frLbNu6FwAuT/HOSI0l2DhVQktTP1FMuK3ie7cA1wCbgc0l+uqq+MT4oyT5gH8CWLVsGemlJEvR7h34K2Dy2vqnbNu4kcKiq/reqvgp8mVHBn6Wq9lfVfFXNz83NnW9mSdIS+hT6vcD2JNuSXALsAQ5NjPk7Ru/OSbKB0SmYE8PFlCRNM7XQq+pJ4EbgLuAB4GBVHU1ya5Jd3bC7gEeSHAPuBv6wqh5ZrdCSpKfqdQ69qg4Dhye23TK2XMBN3UOStAb8pKgkNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDWiV6En2ZnkeJLFJDefY9yrk1SS+eEiSpL6mFroSdYBtwPXAjuAvUl2LDHuucAfAPcMHVKSNF2fd+hXAYtVdaKqngAOALuXGPdW4O3A/wyYT5LUU59C3wg8NLZ+stv2PUmuBDZX1SfO9URJ9iVZSLJw+vTpFYeVJC3vgi+KJnkG8BfAm6aNrar9VTVfVfNzc3MX+tKSpDF9Cv0UsHlsfVO37YznAi8GPpvkQeBq4JAXRiVptvoU+r3A9iTbklwC7AEOndlZVd+sqg1VtbWqtgJHgF1VtbAqiSVJS5pa6FX1JHAjcBfwAHCwqo4muTXJrtUOKEnqZ32fQVV1GDg8se2WZcZec+GxJEkr5SdFJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY3oVehJdiY5nmQxyc1L7L8pybEk9yf5dJLLh48qSTqXqYWeZB1wO3AtsAPYm2THxLAvAPNV9RLgTuDPhw4qSTq3Pu/QrwIWq+pEVT0BHAB2jw+oqrur6vFu9QiwadiYkqRp+hT6RuChsfWT3bblXA98cqkdSfYlWUiycPr06f4pJUlTDXpRNMlrgHngtqX2V9X+qpqvqvm5ubkhX1qSvu+t7zHmFLB5bH1Tt+0sSV4OvAX4xar69jDxJEl99XmHfi+wPcm2JJcAe4BD4wOSXAG8B9hVVQ8PH1OSNM3UQq+qJ4EbgbuAB4CDVXU0ya1JdnXDbgOeA3w0yReTHFrm6SRJq6TPKReq6jBweGLbLWPLLx84lyRphfykqCQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNaJXoSfZmeR4ksUkNy+x/1lJPtLtvyfJ1sGTSpLOaWqhJ1kH3A5cC+wA9ibZMTHseuCxqvoJ4B3A24cOKkk6tz7v0K8CFqvqRFU9ARwAdk+M2Q3c0S3fCbwsSYaLKUmaZn2PMRuBh8bWTwI/t9yYqnoyyTeBHwG+Pj4oyT5gX7f6rSTHzyc0sGHyuS8Sq5YrF/Yzz/fd8bpAF2suuHizmWsF8vYLynX5cjv6FPpgqmo/sP9CnyfJQlXNDxBpUOZaGXOt3MWazVwrs1q5+pxyOQVsHlvf1G1bckyS9cDzgEeGCChJ6qdPod8LbE+yLcklwB7g0MSYQ8Bru+VfBz5TVTVcTEnSNFNPuXTnxG8E7gLWAe+rqqNJbgUWquoQ8DfAB5MsAo8yKv3VdMGnbVaJuVbGXCt3sWYz18qsSq74RlqS2uAnRSWpERa6JDXioir0JM9O8vkkX0pyNMmfLTFm2dsMJHlzt/14klfMONdNSY4luT/Jp5NcPrbvO0m+2D0mLyivdq7XJTk99vq/PbbvtUm+0j1eO/m1q5zrHWOZvpzkG2P7VuV4jT3/uiRfSPLxJfbNfH71zDXz+dUz18znV89cazK/kjyY5N+6515YYn+SvKubR/cnuXJs34Ufr6q6aB5AgOd0y88E7gGunhjze8C7u+U9wEe65R3Al4BnAduA/wDWzTDXLwE/2C3/7plc3fq31vB4vQ74yyW+9gXAie7PS7vlS2eVa2L8GxldbF/V4zX2/DcBHwI+vsS+mc+vnrlmPr965pr5/OqTa63mF/AgsOEc+68DPtl9j1wN3DPk8bqo3qHXyLe61Wd2j8mrtsvdZmA3cKCqvl1VXwUWGd22YCa5quruqnq8Wz3C6Pf1V1XP47WcVwCfqqpHq+ox4FPAzjXKtRf48BCvPU2STcArgfcuM2Tm86tPrrWYX31yncOqza/zyDWz+dXDbuAD3ffIEeD5SS5joON1URU6fO/HqC8CDzP6D7xnYshZtxkAztxmYKlbFGycYa5x1zP6V/iMZydZSHIkya8OlWkFuV7d/Xh3Z5IzHxK7KI5Xd+pgG/CZsc2rdryAdwJ/BHx3mf1rMr965Bo3s/nVM9fM51fPXGsxvwr4xyT3ZXSrk0nLHZdBjtdFV+hV9Z2q+hlG70CuSvLiNY4E9M+V5DXAPHDb2ObLa/Qx398E3pnkx2eY6++BrVX1Ekb/6t/BDKzg73EPcGdVfWds26ocrySvAh6uqvuGeL6hrCTXLOdXz1wzn18r/Huc2fzq/HxVXcno7rQ3JPmFAZ97qouu0M+oqm8Ad/PUHzuWu81An1sUrGYukrwceAuwq6q+PfY1p7o/TwCfBa6YVa6qemQsy3uBn+2W1/x4dfYw8ePwKh6vlwK7kjzI6K6hv5zkbyfGrMX86pNrLebX1FxrNL96Ha/OLOfX+HM/DHyMp56WW+64DHO8hrgQMNQDmAOe3y3/APBPwKsmxtzA2RetDnbLP8XZF61OMNxF0T65rmB0oWz7xPZLgWd1yxuArwA7ZpjrsrHlXwOO1P9fhPlql+/SbvkFs8rV7ftJRheRMovjNfHa17D0Rb6Zz6+euWY+v3rmmvn86pNrLeYX8EPAc8eW/wXYOTHmlZx9UfTzQx6vmd5tsYfLgDsy+p9qPIPRN9PH0+M2AzW6HcFB4BjwJHBDnf1j1mrnug14DvDR0TU0vlZVu4AXAe9J8t3ua99WVcdmmOv3k+xidEweZfRbCVTVo0neyuhePQC3VtWjM8wFo7+7A9XN6M5qHq8lXQTzq0+utZhffXKtxfzqkwtmP79+DPhY9/ezHvhQVf1Dkt8BqKp3A4cZ/abLIvA48Ppu3yDHy4/+S1IjLtpz6JKklbHQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiP+D6V3bGzcl4XKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for bid, sub_df in train_df_full.groupby('business_id'):\n",
    "    if len(sub_df) > 1:\n",
    "        print(sub_df[['business_id', 'funny', 'cool', 'stars']].head())\n",
    "        plt.hist(sub_df['stars'], bins=5)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     user_id  funny  cool  stars\n",
      "1173  -SjQXQd-IRfOdUdYYwWGOQ      0     1      4\n",
      "4503  -SjQXQd-IRfOdUdYYwWGOQ      0     0      1\n"
     ]
    }
   ],
   "source": [
    "for bid, sub_df in train_df_full.groupby('user_id'):\n",
    "    if len(sub_df) > 1:\n",
    "        print(sub_df[['user_id', 'funny', 'cool', 'stars']].head())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baselines\n",
    "\n",
    "Finally, we come up with two baselines for you to refer.\n",
    "We only use text data here and only consider first 5k training samples.\n",
    "\n",
    "For example, a baseline can be a logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [text, stars] columns from the train split\n",
      "succeed!\n",
      "select [text, stars] columns from the valid split\n",
      "succeed!\n"
     ]
    }
   ],
   "source": [
    "train_df = load_data('train')[:5000]\n",
    "valid_df = load_data('valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split above is what we have done for you. You can use the data as you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df['text']\n",
    "y_train = train_df['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tfidf',\n",
      "                 TfidfVectorizer(tokenizer=<function tokenize at 0x7fcfb052e430>)),\n",
      "                ('lr', LogisticRegression())])\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=tokenize)\n",
    "lr = LogisticRegression()\n",
    "steps = [('tfidf', tfidf),('lr', lr)]\n",
    "pipe = Pipeline(steps)\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevyn/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(tokenizer=<function tokenize at 0x7fcfb052e430>)),\n",
       "                ('lr', LogisticRegression())])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      0.88      0.75       517\n",
      "           2       0.41      0.14      0.21       278\n",
      "           3       0.44      0.47      0.45       344\n",
      "           4       0.50      0.51      0.50       427\n",
      "           5       0.70      0.67      0.68       434\n",
      "\n",
      "    accuracy                           0.58      2000\n",
      "   macro avg       0.54      0.53      0.52      2000\n",
      "weighted avg       0.56      0.58      0.56      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[456  23  21  11   6]\n",
      " [119  38  96  20   5]\n",
      " [ 64  22 160  87  11]\n",
      " [ 22   6  78 217 104]\n",
      " [ 32   3  11  99 289]]\n",
      "accuracy 0.58\n"
     ]
    }
   ],
   "source": [
    "x_valid = valid_df['text']\n",
    "y_valid = valid_df['stars']\n",
    "y_pred = pipe.predict(x_valid)\n",
    "print(classification_report(y_valid, y_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(confusion_matrix(y_valid, y_pred))\n",
    "print('accuracy', np.mean(y_valid == y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can use deep learning.\n",
    "Here is a pytorch based baseline using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install torch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_df['text'].map(tokenize).map(filter_stopwords).map(stem)\n",
    "valid_text = valid_df['text'].map(tokenize).map(filter_stopwords).map(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {}\n",
    "for tokens in train_text:\n",
    "    for t in tokens:\n",
    "        if not t in word2id:\n",
    "            word2id[t] = len(word2id)\n",
    "word2id['<pad>'] = len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_id_seq(texts, padding_length=500):\n",
    "    records = []\n",
    "    for tokens in texts:\n",
    "        record = []\n",
    "        for t in tokens:\n",
    "            record.append(word2id.get(t, len(word2id)))\n",
    "        if len(record) >= padding_length:\n",
    "            records.append(record[:padding_length])\n",
    "        else:\n",
    "            records.append(record + [word2id['<pad>']] * (padding_length - len(record)))\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs = texts_to_id_seq(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_seqs = texts_to_id_seq(valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seq, y):\n",
    "        assert len(seq) == len(y)\n",
    "        self.seq = seq\n",
    "        self.y = y-1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return np.asarray(self.seq[idx]), self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(MyDataset(train_seqs, y_train), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(MyDataset(valid_seqs, y_valid), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mlp, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(word2id)+1, embedding_dim=64)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64,\n",
    "                      out_channels=64,\n",
    "                      kernel_size=3,\n",
    "                      stride=1),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=64,\n",
    "                      out_channels=64,\n",
    "                      kernel_size=3,\n",
    "                      stride=1),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.linear = nn.Linear(64, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = self.cnn(x)\n",
    "        x = torch.max(x, dim=-1)[0]\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlp()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/313 [00:00<00:17, 17.38it/s, loss=0.102, acc=0.167] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:14<00:00, 20.95it/s, loss=0.0941, acc=0.337]\n",
      "100%|██████████| 125/125 [00:01<00:00, 99.02it/s] \n",
      "/home/kevyn/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kevyn/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kevyn/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "  1%|          | 3/313 [00:00<00:14, 21.09it/s, loss=0.0937, acc=0.359]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.83      0.58       517\n",
      "           1       0.00      0.00      0.00       278\n",
      "           2       0.33      0.04      0.07       344\n",
      "           3       0.34      0.70      0.46       427\n",
      "           4       0.63      0.19      0.29       434\n",
      "\n",
      "    accuracy                           0.41      2000\n",
      "   macro avg       0.35      0.35      0.28      2000\n",
      "weighted avg       0.38      0.41      0.32      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[427   0   4  78   8]\n",
      " [162   0  10 102   4]\n",
      " [141   0  13 184   6]\n",
      " [ 91   0   7 299  30]\n",
      " [131   0   5 216  82]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:15<00:00, 20.05it/s, loss=0.0788, acc=0.472]\n",
      "100%|██████████| 125/125 [00:01<00:00, 89.19it/s]\n",
      "  1%|          | 3/313 [00:00<00:13, 22.16it/s, loss=0.0544, acc=0.594]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.84      0.62       517\n",
      "           1       0.28      0.18      0.22       278\n",
      "           2       0.41      0.12      0.19       344\n",
      "           3       0.44      0.22      0.29       427\n",
      "           4       0.45      0.65      0.53       434\n",
      "\n",
      "    accuracy                           0.45      2000\n",
      "   macro avg       0.42      0.40      0.37      2000\n",
      "weighted avg       0.43      0.45      0.40      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[436  17   7   6  51]\n",
      " [166  50  13  18  31]\n",
      " [124  69  41  47  63]\n",
      " [ 75  35  25  93 199]\n",
      " [ 79   9  13  49 284]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:14<00:00, 21.30it/s, loss=0.0672, acc=0.57] \n",
      "100%|██████████| 125/125 [00:01<00:00, 97.14it/s] \n",
      "  1%|          | 3/313 [00:00<00:13, 22.27it/s, loss=0.0519, acc=0.719]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.81      0.68       517\n",
      "           1       0.24      0.03      0.05       278\n",
      "           2       0.36      0.31      0.33       344\n",
      "           3       0.42      0.57      0.48       427\n",
      "           4       0.58      0.51      0.54       434\n",
      "\n",
      "    accuracy                           0.50      2000\n",
      "   macro avg       0.44      0.44      0.42      2000\n",
      "weighted avg       0.46      0.50      0.46      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[418   7  30  29  33]\n",
      " [132   8  77  48  13]\n",
      " [ 79  11 105 127  22]\n",
      " [ 28   5  59 245  90]\n",
      " [ 52   3  20 139 220]]\n",
      "epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:14<00:00, 21.94it/s, loss=0.0556, acc=0.645]\n",
      "100%|██████████| 125/125 [00:01<00:00, 89.02it/s]\n",
      "  1%|          | 3/313 [00:00<00:13, 22.50it/s, loss=0.044, acc=0.75]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.67      0.69       517\n",
      "           1       0.44      0.01      0.03       278\n",
      "           2       0.30      0.61      0.40       344\n",
      "           3       0.43      0.38      0.40       427\n",
      "           4       0.55      0.54      0.54       434\n",
      "\n",
      "    accuracy                           0.48      2000\n",
      "   macro avg       0.49      0.44      0.41      2000\n",
      "weighted avg       0.51      0.48      0.45      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[347   3 109  16  42]\n",
      " [ 69   4 167  18  20]\n",
      " [ 41   2 210  66  25]\n",
      " [ 13   0 148 162 104]\n",
      " [ 22   0  62 117 233]]\n",
      "epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:15<00:00, 20.36it/s, loss=0.0431, acc=0.736]\n",
      "100%|██████████| 125/125 [00:01<00:00, 97.09it/s]\n",
      "  1%|          | 3/313 [00:00<00:13, 22.51it/s, loss=0.0217, acc=0.891]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.78      0.69       517\n",
      "           1       0.34      0.13      0.19       278\n",
      "           2       0.36      0.26      0.30       344\n",
      "           3       0.42      0.45      0.43       427\n",
      "           4       0.49      0.61      0.54       434\n",
      "\n",
      "    accuracy                           0.49      2000\n",
      "   macro avg       0.45      0.44      0.43      2000\n",
      "weighted avg       0.46      0.49      0.47      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[401  18  29  19  50]\n",
      " [118  36  62  40  22]\n",
      " [ 68  31  90 101  54]\n",
      " [ 25  11  54 191 146]\n",
      " [ 39   9  17 105 264]]\n",
      "epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:15<00:00, 20.26it/s, loss=0.031, acc=0.833] \n",
      "100%|██████████| 125/125 [00:01<00:00, 91.71it/s]\n",
      "  1%|          | 3/313 [00:00<00:14, 21.29it/s, loss=0.0145, acc=0.938]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.73      0.69       517\n",
      "           1       0.32      0.32      0.32       278\n",
      "           2       0.38      0.31      0.34       344\n",
      "           3       0.42      0.45      0.43       427\n",
      "           4       0.55      0.49      0.52       434\n",
      "\n",
      "    accuracy                           0.49      2000\n",
      "   macro avg       0.46      0.46      0.46      2000\n",
      "weighted avg       0.48      0.49      0.49      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[378  68  19  21  31]\n",
      " [ 89  89  56  26  18]\n",
      " [ 52  69 108  85  30]\n",
      " [ 26  30  82 192  97]\n",
      " [ 38  24  23 136 213]]\n",
      "epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:14<00:00, 21.19it/s, loss=0.0208, acc=0.896]\n",
      "100%|██████████| 125/125 [00:01<00:00, 92.66it/s]\n",
      "  1%|          | 3/313 [00:00<00:14, 21.20it/s, loss=0.0172, acc=0.875]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.59      0.65       517\n",
      "           1       0.34      0.22      0.27       278\n",
      "           2       0.34      0.39      0.36       344\n",
      "           3       0.40      0.44      0.42       427\n",
      "           4       0.49      0.59      0.53       434\n",
      "\n",
      "    accuracy                           0.47      2000\n",
      "   macro avg       0.46      0.45      0.45      2000\n",
      "weighted avg       0.48      0.47      0.47      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[304  65  55  25  68]\n",
      " [ 62  62  90  36  28]\n",
      " [ 28  40 133  98  45]\n",
      " [ 16   8  86 187 130]\n",
      " [ 15   6  31 125 257]]\n",
      "epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:14<00:00, 21.63it/s, loss=0.0126, acc=0.952]\n",
      "100%|██████████| 125/125 [00:01<00:00, 93.40it/s]\n",
      "  1%|          | 3/313 [00:00<00:14, 22.08it/s, loss=0.00628, acc=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.70      0.69       517\n",
      "           1       0.32      0.26      0.29       278\n",
      "           2       0.35      0.31      0.33       344\n",
      "           3       0.44      0.32      0.37       427\n",
      "           4       0.48      0.68      0.56       434\n",
      "\n",
      "    accuracy                           0.49      2000\n",
      "   macro avg       0.45      0.46      0.45      2000\n",
      "weighted avg       0.48      0.49      0.48      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[364  52  26  11  64]\n",
      " [ 86  73  66  22  31]\n",
      " [ 41  62 106  66  69]\n",
      " [ 21  21  87 136 162]\n",
      " [ 23  18  21  76 296]]\n",
      "epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:16<00:00, 19.14it/s, loss=0.00756, acc=0.973]\n",
      "100%|██████████| 125/125 [00:01<00:00, 88.73it/s]\n",
      "  1%|          | 2/313 [00:00<00:18, 16.83it/s, loss=0.00336, acc=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.69      0.69       517\n",
      "           1       0.28      0.44      0.34       278\n",
      "           2       0.33      0.22      0.26       344\n",
      "           3       0.43      0.52      0.47       427\n",
      "           4       0.59      0.41      0.48       434\n",
      "\n",
      "    accuracy                           0.48      2000\n",
      "   macro avg       0.46      0.45      0.45      2000\n",
      "weighted avg       0.49      0.48      0.48      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[356 101  20  19  21]\n",
      " [ 70 121  47  29  11]\n",
      " [ 35 123  75  88  23]\n",
      " [ 23  49  63 221  71]\n",
      " [ 34  38  24 159 179]]\n",
      "epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:15<00:00, 20.44it/s, loss=0.00421, acc=0.988]\n",
      "100%|██████████| 125/125 [00:01<00:00, 92.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.65      0.67       517\n",
      "           1       0.28      0.32      0.30       278\n",
      "           2       0.33      0.29      0.31       344\n",
      "           3       0.41      0.40      0.40       427\n",
      "           4       0.51      0.58      0.54       434\n",
      "\n",
      "    accuracy                           0.47      2000\n",
      "   macro avg       0.45      0.45      0.45      2000\n",
      "weighted avg       0.48      0.47      0.47      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[334  88  33  20  42]\n",
      " [ 71  88  68  30  21]\n",
      " [ 36  80 100  83  45]\n",
      " [ 20  25  82 171 129]\n",
      " [ 17  28  22 116 251]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for e in range(1, 11):    \n",
    "    print('epoch', e)\n",
    "    model.train()\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    with tqdm.tqdm(train_loader) as t:\n",
    "        for x, y in t:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            total_acc += (logits.argmax(1) == y).sum().item()\n",
    "            total_count += y.size(0)\n",
    "            total_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            t.set_postfix({'loss': total_loss/total_count, 'acc': total_acc/total_count})\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    with tqdm.tqdm(valid_loader) as t:\n",
    "        for x, y in t:\n",
    "            logits = model(x)\n",
    "            total_acc += (logits.argmax(1) == y).sum().item()\n",
    "            total_count += len(y)\n",
    "            y_pred += logits.argmax(1).tolist()\n",
    "            y_true += y.tolist()\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"\\n\\n\")\n",
    "    print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning are full of tricks. \n",
    "\n",
    "In the second example above, the implementation of CNN is not good enough to beat even TFIDF+Logistic regression.\n",
    "\n",
    "You can use all the techniques introduced in the lectures and tutorials to enhance your methods.\n",
    "\n",
    "Of course, you can use ideas have not been mentioned to make your model distinguished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
