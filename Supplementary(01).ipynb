{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regular ML.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hFYhWvCKzEi",
        "outputId": "ca1d2cd7-0dae-4966-dcc7-b6f26386fda6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/comp4332-project1"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/comp4332-project1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkmqcd1RhOTI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03456261-5857-4833-8446-1633bf2e9cf6"
      },
      "source": [
        "import graphviz\n",
        "import os\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier, GradientBoostingClassifier\n",
        "from sklearn.datasets import make_classification, load_iris, make_hastie_10_2\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "ps = PorterStemmer()\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stopwords = set(stopwords.words('english'))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeB_A7oEQULh"
      },
      "source": [
        "def load_data(split_name='train', columns=['text', 'stars']):\n",
        "    try:\n",
        "        print(f\"select [{', '.join(columns)}] columns from the {split_name} split\")\n",
        "        df = pd.read_csv(f'data_2021_spring/{split_name}.csv')\n",
        "        df = df.loc[:,columns]\n",
        "        print(\"succeed!\")\n",
        "        return df\n",
        "    except:\n",
        "        print(\"Failed, then try to \")\n",
        "        print(f\"select all columns from the {split_name} split\")\n",
        "        df = pd.read_csv(f'data_2021_spring/{split_name}.csv')\n",
        "        return df"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kLpmPifQXDn",
        "outputId": "56d486c1-112a-4b7d-91c6-3963e0a4f205"
      },
      "source": [
        "train_df = load_data('train', columns=['text','stars'])\n",
        "val_df = load_data('valid', columns=['text','stars'])\n",
        "test_df = load_data('test')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "select [text, stars] columns from the train split\n",
            "succeed!\n",
            "select [text, stars] columns from the valid split\n",
            "succeed!\n",
            "select [text, stars] columns from the test split\n",
            "Failed, then try to \n",
            "select all columns from the test split\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ozut0QwTpFw"
      },
      "source": [
        "def lower(s):\n",
        "    \"\"\"\n",
        "    :param s: a string.\n",
        "    return a string with lower characters\n",
        "    Note that we allow the input to be nested string of a list.\n",
        "    e.g.\n",
        "    Input: 'Text mining is to identify useful information.'\n",
        "    Output: 'text mining is to identify useful information.'\n",
        "    \"\"\"\n",
        "    if isinstance(s, list):\n",
        "        return [lower(t) for t in s]\n",
        "    if isinstance(s, str):\n",
        "        return s.lower()\n",
        "    else:\n",
        "        raise NotImplementedError(\"unknown datatype\")\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    :param text: a doc with multiple sentences, type: str\n",
        "    return a word list, type: list\n",
        "    e.g.\n",
        "    Input: 'Text mining is to identify useful information.'\n",
        "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
        "    \"\"\"\n",
        "    return nltk.word_tokenize(text)\n",
        "\n",
        "\n",
        "def stem(tokens):\n",
        "    \"\"\"\n",
        "    :param tokens: a list of tokens, type: list\n",
        "    return a list of stemmed words, type: list\n",
        "    e.g.\n",
        "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
        "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
        "    \"\"\"\n",
        "    ### equivalent code\n",
        "    # results = list()\n",
        "    # for token in tokens:\n",
        "    #     results.append(ps.stem(token))\n",
        "    # return results\n",
        "\n",
        "    return [ps.stem(token) for token in tokens]\n",
        "\n",
        "def n_gram(tokens, n= 3):\n",
        "    \"\"\"\n",
        "    :param tokens: a list of tokens, type: list\n",
        "    :param n: the corresponding n-gram, type: int\n",
        "    return a list of n-gram tokens, type: list\n",
        "    e.g.\n",
        "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n",
        "    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n",
        "    \"\"\"\n",
        "    if n == 1:\n",
        "        return tokens\n",
        "    else:\n",
        "        results = list()\n",
        "        for i in range(len(tokens)-n+1):\n",
        "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
        "            results.append(\" \".join(tokens[i:i+n]))\n",
        "        return results\n",
        "\n",
        "def filter_stopwords(tokens):\n",
        "    \"\"\"\n",
        "    :param tokens: a list of tokens, type: list\n",
        "    return a list of filtered tokens, type: list\n",
        "    e.g.\n",
        "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
        "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
        "    \"\"\"\n",
        "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]\n",
        "\n",
        "def get_onehot_vector(feats, feats_dict):\n",
        "    \"\"\"\n",
        "    :param data: a list of features, type: list\n",
        "    :param feats_dict: a dict from features to indices, type: dict\n",
        "    return a feature vector,\n",
        "    \"\"\"\n",
        "    # initialize the vector as all zeros\n",
        "    vector = np.zeros(len(feats_dict), dtype=np.float)\n",
        "    for f in feats:\n",
        "        # get the feature index, return -1 if the feature is not existed\n",
        "        f_idx = feats_dict.get(f, -1)\n",
        "        if f_idx != -1:\n",
        "            # set the corresponding element as 1\n",
        "            vector[f_idx] = 1\n",
        "    return vector\n",
        "\n",
        "\n",
        "def get_feats_dict(feats, min_freq=-1, max_freq=-1, max_size=-1):\n",
        "    \"\"\"\n",
        "    :param data: a list of features, type: list(list)\n",
        "    :param min_freq: the lowest fequency that the fequency of a feature smaller than it will be filtered out, type: int\n",
        "    :param max_freq: the highest fequency that the fequency of a feature larger than it will be filtered out, type: int\n",
        "    :param max_size: the max size of feature dict, type: int\n",
        "    return a feature dict that maps features to indices, sorted by frequencies\n",
        "    # Counter document: https://docs.python.org/3.6/library/collections.html#collections.Counter\n",
        "    \"\"\"\n",
        "    # count all features\n",
        "    feat_cnt = Counter(feats) # [\"text\", \"text\", \"mine\"] --> {\"text\": 2, \"mine\": 1}\n",
        "    if max_size > 0 and min_freq == -1 and max_freq == -1:\n",
        "        valid_feats = [f for f, cnt in feat_cnt.most_common(max_size)]\n",
        "    else:\n",
        "        valid_feats = list()\n",
        "        for f, cnt in feat_cnt.most_common():\n",
        "            if (min_freq == -1 or cnt >= min_freq) and \\\n",
        "                (max_freq == -1 or cnt <= max_freq):\n",
        "                valid_feats.append(f)\n",
        "    if max_size > 0 and len(valid_feats) > max_size:\n",
        "        valid_feats = valid_feats[:max_size]        \n",
        "    print(\"Size of features:\", len(valid_feats))\n",
        "    \n",
        "    # build a mapping from features to indices\n",
        "    feats_dict = dict(zip(valid_feats, range(len(valid_feats))))\n",
        "    return feats_dict\n",
        "    \n",
        "def get_index_vector(feats, feats_dict, max_len):\n",
        "    \"\"\"\n",
        "    :param feats: a list of features, type: list\n",
        "    :param feats_dict: a dict from features to indices, type: dict\n",
        "    :param feats: a list of features, type: list\n",
        "    return a feature vector,\n",
        "    \"\"\"\n",
        "    # initialize the vector as all zeros\n",
        "    vector = np.zeros(max_len, dtype=np.int64)\n",
        "    for i, f in enumerate(feats):\n",
        "        if i == max_len:\n",
        "            break\n",
        "        # get the feature index, return 1 (<unk>) if the feature is not existed\n",
        "        f_idx = feats_dict.get(f, 1)\n",
        "        vector[i] = f_idx\n",
        "    return vector"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbBHAdDmU19E"
      },
      "source": [
        "train_df['tokens'] = train_df['text'].map(tokenize).map(filter_stopwords).map(lower)\n",
        "x_train = train_df['text']\n",
        "y_train = train_df['stars']"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXzOsLH1ayxy"
      },
      "source": [
        "## Some traditional machine learning approaches\n",
        "Note that Logistic Regression is implemented in another .ipynb file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGL1DlAabI8f"
      },
      "source": [
        "SVM Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxDHdqrLbH4R",
        "outputId": "bbe6c98b-3764-4db6-c1b6-184ae8221ad8"
      },
      "source": [
        "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words={'english'})\n",
        "model = SVC(C=1.0, kernel='sigmoid', degree=3 , gamma='auto', coef0=0.0,\n",
        "          shrinking=True,\n",
        "          probability=False, tol=0.001, cache_size=200, class_weight=None,\n",
        "          verbose=False,\n",
        "          max_iter=-1, decision_function_shape='ovr', random_state=None)\n",
        "steps = [('tfidf', tfidf),('model', model)]\n",
        "pipe = Pipeline(steps)\n",
        "# print(pipe)\n",
        "pipe.fit(x_train, y_train)\n",
        "x_valid = val_df['text']\n",
        "y_valid = val_df['stars']\n",
        "y_pred = pipe.predict(x_valid)\n",
        "print(classification_report(y_valid, y_pred))\n",
        "print(\"\\n\\n\")\n",
        "print(confusion_matrix(y_valid, y_pred))\n",
        "print('accuracy', np.mean(y_valid == y_pred))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.26      1.00      0.41       517\n",
            "           2       0.00      0.00      0.00       278\n",
            "           3       0.00      0.00      0.00       344\n",
            "           4       0.00      0.00      0.00       427\n",
            "           5       0.00      0.00      0.00       434\n",
            "\n",
            "    accuracy                           0.26      2000\n",
            "   macro avg       0.05      0.20      0.08      2000\n",
            "weighted avg       0.07      0.26      0.11      2000\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[[517   0   0   0   0]\n",
            " [278   0   0   0   0]\n",
            " [344   0   0   0   0]\n",
            " [427   0   0   0   0]\n",
            " [434   0   0   0   0]]\n",
            "accuracy 0.2585\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6CbX_7-bXIn"
      },
      "source": [
        "KNN Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9E_46ollbWjg",
        "outputId": "9bf45a1a-9365-41e8-a9b6-0a89fdc3d323"
      },
      "source": [
        "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words={'english'})\n",
        "model = KNeighborsClassifier(n_neighbors=6, weights='uniform', algorithm='auto',\n",
        "                           leaf_size=50, p=2, metric='minkowski', \n",
        "                           metric_params=None, n_jobs=1)\n",
        "steps = [('tfidf', tfidf),('model', model)]\n",
        "pipe = Pipeline(steps)\n",
        "# print(pipe)\n",
        "pipe.fit(x_train, y_train)\n",
        "x_valid = val_df['text']\n",
        "y_valid = val_df['stars']\n",
        "y_pred = pipe.predict(x_valid)\n",
        "print(classification_report(y_valid, y_pred))\n",
        "print(\"\\n\\n\")\n",
        "print(confusion_matrix(y_valid, y_pred))\n",
        "print('accuracy', np.mean(y_valid == y_pred))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.44      0.83      0.58       517\n",
            "           2       0.20      0.18      0.19       278\n",
            "           3       0.28      0.23      0.25       344\n",
            "           4       0.36      0.28      0.31       427\n",
            "           5       0.61      0.22      0.32       434\n",
            "\n",
            "    accuracy                           0.39      2000\n",
            "   macro avg       0.38      0.35      0.33      2000\n",
            "weighted avg       0.40      0.39      0.36      2000\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[[429  33  24  19  12]\n",
            " [149  51  48  28   2]\n",
            " [119  66  79  69  11]\n",
            " [117  64  91 118  37]\n",
            " [152  45  44  97  96]]\n",
            "accuracy 0.3865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlLVIajIaUqN"
      },
      "source": [
        "## Tree Methos\n",
        "\n",
        "We will try some tree method in the following part of the code:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57IMEoWUbxsF"
      },
      "source": [
        "Decision Tree Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4U-RQjJ9b3Ia",
        "outputId": "61a3c5db-e5a5-4bdc-c44c-2441f2b8bcc5"
      },
      "source": [
        "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words={'english'})\n",
        "model = DecisionTreeClassifier(criterion='gini',splitter='best',max_depth=None,\n",
        "                             min_samples_split=2,min_samples_leaf=1,\n",
        "                             min_weight_fraction_leaf=0.0,max_features=None,\n",
        "                             random_state=None,max_leaf_nodes=None,\n",
        "                             min_impurity_decrease=0.0,min_impurity_split=1e-07,\n",
        "                             class_weight=None, presort=False)\n",
        "steps = [('tfidf', tfidf),('model', model)]\n",
        "pipe = Pipeline(steps)\n",
        "# print(pipe)\n",
        "pipe.fit(x_train, y_train)\n",
        "x_valid = val_df['text']\n",
        "y_valid = val_df['stars']\n",
        "y_pred = pipe.predict(x_valid)\n",
        "print(classification_report(y_valid, y_pred))\n",
        "print(\"\\n\\n\")\n",
        "print(confusion_matrix(y_valid, y_pred))\n",
        "print('accuracy', np.mean(y_valid == y_pred))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py:319: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.56      0.58      0.57       517\n",
            "           2       0.26      0.23      0.24       278\n",
            "           3       0.27      0.29      0.28       344\n",
            "           4       0.32      0.31      0.31       427\n",
            "           5       0.46      0.44      0.45       434\n",
            "\n",
            "    accuracy                           0.39      2000\n",
            "   macro avg       0.37      0.37      0.37      2000\n",
            "weighted avg       0.39      0.39      0.39      2000\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[[301  60  59  49  48]\n",
            " [ 86  63  63  42  24]\n",
            " [ 73  60 101  73  37]\n",
            " [ 47  41  95 134 110]\n",
            " [ 35  22  61 126 190]]\n",
            "accuracy 0.3945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOxhEMO5dME7"
      },
      "source": [
        "Adaboost Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViRHczX_dLtE",
        "outputId": "fa707205-6bf1-47aa-ab72-b735afd34c38"
      },
      "source": [
        "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words={'english'})\n",
        "model = AdaBoostClassifier(algorithm='SAMME.R', learning_rate=1e-1, n_estimators=100, random_state=2)\n",
        "steps = [('tfidf', tfidf),('model', model)]\n",
        "pipe = Pipeline(steps)\n",
        "# print(pipe)\n",
        "pipe.fit(x_train, y_train)\n",
        "x_valid = val_df['text']\n",
        "y_valid = val_df['stars']\n",
        "y_pred = pipe.predict(x_valid)\n",
        "print(classification_report(y_valid, y_pred))\n",
        "print(\"\\n\\n\")\n",
        "print(confusion_matrix(y_valid, y_pred))\n",
        "print('accuracy', np.mean(y_valid == y_pred))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.56      0.84      0.67       517\n",
            "           2       0.50      0.00      0.01       278\n",
            "           3       0.37      0.43      0.40       344\n",
            "           4       0.43      0.47      0.45       427\n",
            "           5       0.63      0.54      0.58       434\n",
            "\n",
            "    accuracy                           0.51      2000\n",
            "   macro avg       0.50      0.46      0.42      2000\n",
            "weighted avg       0.51      0.51      0.47      2000\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[[432   1  42  26  16]\n",
            " [138   1  95  33  11]\n",
            " [ 92   0 148  91  13]\n",
            " [ 55   0  77 201  94]\n",
            " [ 49   0  34 118 233]]\n",
            "accuracy 0.5075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3C93UIzb_w6"
      },
      "source": [
        "Gradient Boosting Classification (GBDT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fYJj3Rqb_ZL",
        "outputId": "843964d8-54bc-4e06-d172-286b129137c0"
      },
      "source": [
        "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words={'english'})\n",
        "model = GradientBoostingClassifier(loss='deviance',learning_rate=0.1,n_estimators=100,subsample=1.0,criterion='friedman_mse',\n",
        "                                 min_samples_split=2,min_samples_leaf=1,\n",
        "                                 min_weight_fraction_leaf=0.0,max_depth=3,min_impurity_decrease=0.0,min_impurity_split=None,\n",
        "                                 init=None,random_state=None,max_features=None,verbose=0,max_leaf_nodes=None,\n",
        "                                 warm_start=False,presort='auto',\n",
        "                                 validation_fraction=0.1,n_iter_no_change=None, tol=0.0001) \n",
        "steps = [('tfidf', tfidf),('model', model)]\n",
        "pipe = Pipeline(steps)\n",
        "# print(pipe)\n",
        "pipe.fit(x_train, y_train)\n",
        "x_valid = val_df['text']\n",
        "y_valid = val_df['stars']\n",
        "y_pred = pipe.predict(x_valid)\n",
        "print(classification_report(y_valid, y_pred))\n",
        "print(\"\\n\\n\")\n",
        "print(confusion_matrix(y_valid, y_pred))\n",
        "print('accuracy', np.mean(y_valid == y_pred))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.66      0.84      0.74       517\n",
            "           2       0.37      0.14      0.20       278\n",
            "           3       0.43      0.46      0.45       344\n",
            "           4       0.44      0.45      0.45       427\n",
            "           5       0.64      0.64      0.64       434\n",
            "\n",
            "    accuracy                           0.55      2000\n",
            "   macro avg       0.51      0.51      0.49      2000\n",
            "weighted avg       0.53      0.55      0.53      2000\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[[433  21  30  17  16]\n",
            " [113  38  79  37  11]\n",
            " [ 56  31 159  79  19]\n",
            " [ 28   7  85 193 114]\n",
            " [ 25   5  17 108 279]]\n",
            "accuracy 0.551\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMpYVtxkah1U"
      },
      "source": [
        "Random Forest:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYs8JBnqXmZa",
        "outputId": "c2a96839-50b9-46cc-8428-7e59095c3a27"
      },
      "source": [
        "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words={'english'})\n",
        "model = RandomForestClassifier(criterion='entropy',n_jobs=-1)\n",
        "steps = [('tfidf', tfidf),('model', model)]\n",
        "pipe = Pipeline(steps)\n",
        "# print(pipe)\n",
        "pipe.fit(x_train, y_train)\n",
        "x_valid = val_df['text']\n",
        "y_valid = val_df['stars']\n",
        "y_pred = pipe.predict(x_valid)\n",
        "print(classification_report(y_valid, y_pred))\n",
        "print(\"\\n\\n\")\n",
        "print(confusion_matrix(y_valid, y_pred))\n",
        "print('accuracy', np.mean(y_valid == y_pred))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.54      0.92      0.68       517\n",
            "           2       0.36      0.01      0.03       278\n",
            "           3       0.43      0.30      0.35       344\n",
            "           4       0.44      0.42      0.43       427\n",
            "           5       0.61      0.66      0.63       434\n",
            "\n",
            "    accuracy                           0.52      2000\n",
            "   macro avg       0.48      0.46      0.43      2000\n",
            "weighted avg       0.49      0.52      0.47      2000\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[[476   1  15   7  18]\n",
            " [162   4  51  44  17]\n",
            " [115   5 103  96  25]\n",
            " [ 66   0  60 179 122]\n",
            " [ 55   1  11  81 286]]\n",
            "accuracy 0.524\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLeOhDqkcB_m"
      },
      "source": [
        "Xgboost Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7AZfzqFdGkk",
        "outputId": "47ae9b12-fca5-4b39-9b81-6c34b4ae154c"
      },
      "source": [
        "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words={'english'})\n",
        "model = XGBClassifier(learning_rate=0.05, n_estimators=100, silent=True, \n",
        "                    objective='multi:softmax', num_class=5, booster='gbtree', n_jobs=-1,\n",
        "                    subsample=1, colsample_bytree=1, max_depth=12,\n",
        "                    colsample_bylevel=1, reg_alpha=2, reg_lambda=2, scale_pos_weight=1,\n",
        "                    base_score=0.5,\n",
        "                    random_state=0, seed=None, missing=None)\n",
        "steps = [('tfidf', tfidf),('model', model)]\n",
        "pipe = Pipeline(steps)\n",
        "# print(pipe)\n",
        "pipe.fit(x_train, y_train)\n",
        "x_valid = val_df['text']\n",
        "y_valid = val_df['stars']\n",
        "y_pred = pipe.predict(x_valid)\n",
        "print(classification_report(y_valid, y_pred))\n",
        "print(\"\\n\\n\")\n",
        "print(confusion_matrix(y_valid, y_pred))\n",
        "print('accuracy', np.mean(y_valid == y_pred))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.65      0.87      0.74       517\n",
            "           2       0.35      0.12      0.18       278\n",
            "           3       0.42      0.42      0.42       344\n",
            "           4       0.47      0.50      0.49       427\n",
            "           5       0.67      0.62      0.64       434\n",
            "\n",
            "    accuracy                           0.56      2000\n",
            "   macro avg       0.51      0.51      0.49      2000\n",
            "weighted avg       0.53      0.56      0.53      2000\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[[451  22  19  12  13]\n",
            " [121  33  85  29  10]\n",
            " [ 56  31 143  94  20]\n",
            " [ 40   5  74 215  93]\n",
            " [ 31   4  21 108 270]]\n",
            "accuracy 0.556\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}